import random
import json
import os
from app import schemas

def load_json_questions(filename: str):
    path = os.path.join(os.path.dirname(__file__), f"../data/questions/{filename}")
    if os.path.exists(path):
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    return []

async def generate_practice_question(topic_id: int, difficulty: str) -> schemas.GeneratedQuestion:
    # Simulating Topic ID 1 = Statistics 1 Week 1
    # In a real app we'd map topic_id to filenames
    
    questions = []
    if topic_id == 1:
        questions = load_json_questions("stats1_week1.json")
    
    if questions:
        q = random.choice(questions)
        # Parse options if we can or just return as is. 
        # Our parser put everything in question_text, so options list might be dummy or empty.
        # For the UI to work, we need an options array. 
        # Let's create dummy options if not present or parse real ones if our parser was smarter.
        # For now, let's assume the question_text has everything and we return generic ABCD options for the UI to be clickable, 
        # OR we just put the text in options.
        # Ideally we should fix the parser to extract options, but as a fallback/MVP:
        
        return schemas.GeneratedQuestion(
            question_text=q["question_text"],
            options=["Option A (See text)", "Option B (See text)", "Option C (See text)", "Option D (See text)"], 
            correct_option=0, # We don't know the index without better parsing
            explanation=f"Correct Answer: {q['correct_answer']}\n\nExplanation: {q['explanation']}"
        )

    # Fallback Mock
    return schemas.GeneratedQuestion(
        question_text=f"Sample Question for Topic {topic_id} ({difficulty})",
        options=["Option A", "Option B", "Option C", "Option D"],
        correct_option=0,
        explanation="This is a mock explanation generated by the system."
    )

async def evaluate_student_answer(question: str, answer: str) -> schemas.EvaluationResult:
    # Mock implementation
    return schemas.EvaluationResult(
        score=0.85,
        feedback="Good attempt! logic is sound but calculation has minor error.",
        improvement_areas=["Check arithmetic", "Review basic concepts"]
    )
